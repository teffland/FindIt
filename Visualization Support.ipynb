{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Take all meta data and convert to one graph file, for a domain subdirectory"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# NOTE: BEFORE RUNNING THIS CODE, THE BACKLINKS AND RELEVANCE SCORING ALGORITHMS SHOULD BE RUN\n",
      "import sys\n",
      "import os\n",
      "import json\n",
      "import random\n",
      "\n",
      "domain = \"buffalo\"\n",
      "data_dir = \"data/\"+domain\n",
      "\n",
      "# grab all the metadata files\n",
      "files = os.listdir(data_dir)\n",
      "node_files = [file for file in files if file[-5:] == \"_meta\"]\n",
      "random.shuffle(node_files)\n",
      "# make the big graph dict\n",
      "graph = {}\n",
      "nodes_index = {}\n",
      "graph['nodes'] = []\n",
      "graph['links'] = []\n",
      "\n",
      "# setup nodes array\n",
      "for fname in node_files:\n",
      "    f = open(data_dir+'/'+fname, 'r')\n",
      "    data = json.load(f)\n",
      "    \n",
      "    #if data['depth'] > 5: continue \n",
      "    # get node info\n",
      "    node = {\"url\":data['url'], \"relevance\":data['relevance'], \"parents\":[], \"children\":[]}\n",
      "    graph['nodes'].append(node)\n",
      "    \n",
      "    nodes_index[node['url']] = graph['nodes'].index(node)\n",
      "    #print 'Node: ', node['url'], \" w/ index: \", nodes_index[node['url']] \n",
      "    f.close()\n",
      "    \n",
      "#print \"Node index: \",nodes_index\n",
      "# setup links array (source and target must be indices of nodes in nodes array)\n",
      "# we also only want links to recorded nodes\n",
      "for fname in node_files:\n",
      "    f = open(data_dir+'/'+fname, 'r')\n",
      "    data = json.load(f)\n",
      "    source = data['url']\n",
      "        \n",
      "    # get link info\n",
      "    for link in data['outLinks']:\n",
      "        target = link['url']\n",
      "        if (target in nodes_index.keys()) and (source in nodes_index.keys()) and (nodes_index[source] != nodes_index[target]):\n",
      "            edge = {\"source\":nodes_index[source], \"target\":nodes_index[target]}\n",
      "            #print edge\n",
      "            # add source as parent of target \n",
      "            graph['nodes'][nodes_index[target]]['parents'].append(nodes_index[source])\n",
      "            \n",
      "            # add target as child of source\n",
      "            graph['nodes'][nodes_index[source]]['children'].append(nodes_index[target])\n",
      "            \n",
      "            graph['links'].append(edge)\n",
      "    \n",
      "    f.close()\n",
      "#for node in graph['nodes']:\n",
      "#    print int(node['url'])-1, node['parents'], node['children']\n",
      "# write out big graph to one file (easier for D3 to load)\n",
      "#print graph\n",
      "f = open(data_dir+'/graph.json', 'w')\n",
      "f.write(json.dumps(dict(graph), indent=4) + \"\\n\")\n",
      "f.close()\n",
      "print \"Done\"\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Done\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}