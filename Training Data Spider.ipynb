{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!ls -R"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Training Data Spider.ipynb scrapy.cfg                 \u001b[34mthesis\u001b[m\u001b[m\r\n",
        "\r\n",
        "./thesis:\r\n",
        "__init__.py  __init__.pyc items.py     items.pyc    pipelines.py settings.py  settings.pyc \u001b[34mspiders\u001b[m\u001b[m\r\n",
        "\r\n",
        "./thesis/spiders:\r\n",
        "__init__.py     __init__.pyc    basicSpider.py  basicSpider.pyc\r\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Crawl Item Definitions"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%writefile data/buffalo/target_urls.json\n",
      "\n",
      "{ \"target_urls\":[\"http://www.cse.buffalo.edu/graduate/courses.php\",\n",
      "                \"http://undergrad-catalog.buffalo.edu/coursedescriptions/index.php?frm_abbr=CSE&frm_num=101\",\n",
      "                \"http://undergrad-catalog.buffalo.edu/coursedescriptions/index.php?frm_abbr=CSE&frm_num=487\",\n",
      "                \"http://undergrad-catalog.buffalo.edu/academicprograms/mth_courses.shtml\",\n",
      "                \"http://undergrad-catalog.buffalo.edu/academicprograms/his_courses.shtml\",\n",
      "                \"http://undergrad-catalog.buffalo.edu/academicprograms/lin_courses.shtml\"\n",
      "                ],\n",
      " \n",
      " \"target_paths\":[[\"http://www.buffalo.edu/\",\"http://www.buffalo.edu/academics.html\",\"http://admissions.buffalo.edu/academics/areasofstudy.php\",\"http://admissions.buffalo.edu/academics/programs/mth.php\",\"http://undergrad-catalog.buffalo.edu/academicprograms/mth_courses.shtml\"],\n",
      "                 [\"http://www.buffalo.edu/\",\"http://www.buffalo.edu/academics.html\",\"http://admissions.buffalo.edu/academics/areasofstudy.php\",\"http://admissions.buffalo.edu/academics/programs/ase.php\",\"http://undergrad-catalog.buffalo.edu/academicprograms/ase_courses.shtml\"],\n",
      "                 [\"http://www.buffalo.edu/\",\"http://www.buffalo.edu/academics.html\",\"http://admissions.buffalo.edu/academics/areasofstudy.php\",\"http://admissions.buffalo.edu/academics/programs/acc.php\",\"http://undergrad-catalog.buffalo.edu/academicprograms/acc_courses.shtml\"],\n",
      "                 [\"http://www.buffalo.edu/\",\"http://www.buffalo.edu/academics.html\",\"http://admissions.buffalo.edu/academics/areasofstudy.php\",\"http://admissions.buffalo.edu/academics/programs/aas.php\",\"http://undergrad-catalog.buffalo.edu/academicprograms/aas_courses.shtml\"],\n",
      "                 [\"http://www.buffalo.edu/\",\"http://www.buffalo.edu/academics.html\",\"http://admissions.buffalo.edu/academics/areasofstudy.php\",\"http://admissions.buffalo.edu/academics/programs/ams.php\",\"http://undergrad-catalog.buffalo.edu/academicprograms/ams_courses.shtml\"],\n",
      "                 [\"http://www.buffalo.edu/\",\"http://www.buffalo.edu/academics.html\",\"http://admissions.buffalo.edu/academics/areasofstudy.php\",\"http://admissions.buffalo.edu/academics/programs/apy.php\",\"http://undergrad-catalog.buffalo.edu/academicprograms/apy_courses.shtml\"],\n",
      "                 [\"http://www.buffalo.edu/\",\"http://www.buffalo.edu/academics.html\",\"http://admissions.buffalo.edu/academics/areasofstudy.php\",\"http://admissions.buffalo.edu/academics/programs/arc.php\",\"http://undergrad-catalog.buffalo.edu/academicprograms/arc_courses.shtml\"],\n",
      "                 [\"http://www.buffalo.edu/\",\"http://www.buffalo.edu/academics.html\",\"http://admissions.buffalo.edu/academics/areasofstudy.php\",\"http://admissions.buffalo.edu/academics/programs/art.php\",\"http://undergrad-catalog.buffalo.edu/academicprograms/art_courses.shtml\"],\n",
      "                 [\"http://www.buffalo.edu/\",\"http://www.buffalo.edu/academics.html\",\"http://admissions.buffalo.edu/academics/areasofstudy.php\",\"http://admissions.buffalo.edu/academics/programs/ahi.php\",\"http://undergrad-catalog.buffalo.edu/academicprograms/ahi_courses.shtml\"],\n",
      "                 [\"http://www.buffalo.edu/\",\"http://www.buffalo.edu/academics.html\",\"http://admissions.buffalo.edu/academics/areasofstudy.php\",\"http://admissions.buffalo.edu/academics/programs/as.php\",\"http://undergrad-catalog.buffalo.edu/academicprograms/as_courses.shtml\"],\n",
      "                 [\"http://www.buffalo.edu/\",\"http://www.buffalo.edu/academics.html\",\"http://admissions.buffalo.edu/academics/areasofstudy.php\",\"http://admissions.buffalo.edu/academics/programs/bch.php\",\"http://undergrad-catalog.buffalo.edu/academicprograms/bch_courses.shtml\"],\n",
      "                 [\"http://www.buffalo.edu/\",\"http://www.buffalo.edu/academics.html\",\"http://admissions.buffalo.edu/academics/areasofstudy.php\",\"http://admissions.buffalo.edu/academics/programs/bioinfo.php\",\"http://undergrad-catalog.buffalo.edu/academicprograms/bioinfo_courses.shtml\"],\n",
      "                 [\"http://www.buffalo.edu/\",\"http://www.buffalo.edu/academics.html\",\"http://admissions.buffalo.edu/academics/areasofstudy.php\",\"http://admissions.buffalo.edu/academics/programs/bio.php\",\"http://undergrad-catalog.buffalo.edu/academicprograms/bio_courses.shtml\"],\n",
      "                 [\"http://www.buffalo.edu/\",\"http://www.buffalo.edu/academics.html\",\"http://admissions.buffalo.edu/academics/areasofstudy.php\",\"http://admissions.buffalo.edu/academics/programs/bme.php\",\"http://undergrad-catalog.buffalo.edu/academicprograms/bme_courses.shtml\"],\n",
      "                 [\"http://www.buffalo.edu/\",\"http://www.buffalo.edu/academics.html\",\"http://admissions.buffalo.edu/academics/areasofstudy.php\",\"http://admissions.buffalo.edu/academics/programs/bms.php\",\"http://undergrad-catalog.buffalo.edu/academicprograms/bms_courses.shtml\"],\n",
      "                 [\"http://www.buffalo.edu/\",\"http://www.buffalo.edu/academics.html\",\"http://admissions.buffalo.edu/academics/areasofstudy.php\",\"http://admissions.buffalo.edu/academics/programs/btc.php\",\"http://undergrad-catalog.buffalo.edu/academicprograms/btc_courses.shtml\"],\n",
      "                 [\"http://www.buffalo.edu/\",\"http://www.buffalo.edu/academics.html\",\"http://admissions.buffalo.edu/academics/areasofstudy.php\",\"http://admissions.buffalo.edu/academics/programs/mgt.php\",\"http://undergrad-catalog.buffalo.edu/academicprograms/mgt_courses.shtml\"],\n",
      "                 [\"http://www.buffalo.edu/\",\"http://www.buffalo.edu/academics.html\",\"http://admissions.buffalo.edu/academics/areasofstudy.php\",\"http://admissions.buffalo.edu/academics/programs/ce.php\",\"http://undergrad-catalog.buffalo.edu/academicprograms/ce_courses.shtml\"],\n",
      "                 [\"http://www.buffalo.edu/\",\"http://www.buffalo.edu/academics.html\",\"http://admissions.buffalo.edu/academics/areasofstudy.php\",\"http://admissions.buffalo.edu/academics/programs/che.php\",\"http://undergrad-catalog.buffalo.edu/academicprograms/che_courses.shtml\"],\n",
      "                 [\"http://www.buffalo.edu/\",\"http://www.buffalo.edu/academics.html\",\"http://admissions.buffalo.edu/academics/areasofstudy.php\",\"http://admissions.buffalo.edu/academics/programs/cie.php\",\"http://undergrad-catalog.buffalo.edu/academicprograms/cie_courses.shtml\"],\n",
      "                 [\"http://www.buffalo.edu/\",\"http://www.buffalo.edu/academics.html\",\"http://admissions.buffalo.edu/academics/areasofstudy.php\",\"http://admissions.buffalo.edu/academics/programs/cl.php\",\"http://undergrad-catalog.buffalo.edu/academicprograms/cl_courses.shtml\"],\n",
      "                 [\"http://www.buffalo.edu/\",\"http://www.buffalo.edu/academics.html\",\"http://admissions.buffalo.edu/academics/areasofstudy.php\",\"http://admissions.buffalo.edu/academics/programs/cogsci.php\",\"http://undergrad-catalog.buffalo.edu/academicprograms/cogsci_courses.shtml\"],\n",
      "                 [\"http://www.buffalo.edu/\",\"http://www.buffalo.edu/academics.html\",\"http://admissions.buffalo.edu/academics/areasofstudy.php\",\"http://admissions.buffalo.edu/academics/programs/com.php\",\"http://undergrad-catalog.buffalo.edu/academicprograms/com_courses.shtml\"],\n",
      "                 [\"http://www.buffalo.edu/\",\"http://www.buffalo.edu/academics.html\",\"http://admissions.buffalo.edu/academics/areasofstudy.php\",\"http://admissions.buffalo.edu/academics/programs/comphys.php\",\"http://undergrad-catalog.buffalo.edu/academicprograms/comphys_courses.shtml\"]\n",
      "                ]\n",
      " }"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Overwriting data/buffalo/target_urls.json\n"
       ]
      }
     ],
     "prompt_number": 20
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%writefile findit/items.py\n",
      "#############################\n",
      "# -*- coding: utf-8 -*-\n",
      "\n",
      "# Define here the models for your scraped items\n",
      "#\n",
      "# See documentation in:\n",
      "# http://doc.scrapy.org/en/latest/topics/items.html\n",
      "\n",
      "import scrapy as s\n",
      "\n",
      "\"\"\"\n",
      "Main Webpage class for initial exploration\n",
      "\"\"\"\n",
      "class WebPageItem(s.Item):\n",
      "    # file meta data\n",
      "    url = s.Field() # page url\n",
      "    depth = s.Field() # how many layers away from root we are\n",
      "    target = s.Field() # boolean for whether this is a target page or not\n",
      "    \n",
      "    # webgraph info\n",
      "    inLinks = s.Field() # a list of (href, anchor-text) pairs\n",
      "    outLinks = s.Field() # a list of (href, anchor-text) pairs\n",
      "    # filegraph info\n",
      "    inFiles = s.Field() # inLinks with href as hashed filename\n",
      "    outFiles = s.Field() # outLinks with href as hashed filename\n",
      "    \n",
      "    # webpage content\n",
      "    title = s.Field() # title of page\n",
      "    headers = s.Field() # Response.headers\n",
      "    body = s.Field() # body text of page\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Overwriting findit/items.py\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Training Data Spider Defintion:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%writefile findit/spiders/trainDataGatherer.py\n",
      "###############################################\n",
      "import scrapy as s\n",
      "from bs4 import BeautifulSoup as bs\n",
      "import json\n",
      "from findit.items import WebPageItem\n",
      "\n",
      "class trainDataGatherer(s.Spider):\n",
      "    name = \"gather_data\"\n",
      "\n",
      "    def __init__(self, domain=\"buffalo.edu\", data=\"buffalo\", *a, **kw):\n",
      "        self.allowed_domains = [domain]\n",
      "        self.start_urls = [ \"http://www.buffalo.edu/academics.html\" ]\n",
      "        self.data_dir = 'data/'+data\n",
      "        try:\n",
      "            f = open(self.data_dir + '/target_urls.json', 'r')\n",
      "        except FileNotFoundError:\n",
      "            print \"No target URLs specified, proceeding with no targets.\"\n",
      "            self.target_urls = []\n",
      "        #print f.readlines()\n",
      "        data = json.load(f)\n",
      "        self.target_urls = data['target_urls']\n",
      "        # pick off target path ends as targets\n",
      "        self.target_urls.append([path[-1] for path in data['target_paths']])\n",
      "        self.target_paths = data['target_paths']\n",
      "        self.targets_set = set([page for path in self.target_paths for page in path])\n",
      "        \n",
      "        print \"Target URLs: \", self.target_urls\n",
      "  \n",
      "        super(trainDataGatherer, self).__init__(*a, **kw)\n",
      "    \n",
      "    def parse(self, response):\n",
      "        # parse the page with bs4\n",
      "        p = bs(response._body)\n",
      "\n",
      "        # the page item\n",
      "        page  = WebPageItem()\n",
      "        links = []\n",
      "        # mark target pages\n",
      "        if response._url in self.target_urls: page['target'] = True\n",
      "        else: page['target'] = False\n",
      "        \n",
      "        # standardize current page url\n",
      "        current_url = self.make_canonical_url(response._url)\n",
      "        \n",
      "        # populate page data item\n",
      "        page['url'] = current_url # add url to page object\n",
      "        page['headers'] = response.headers\n",
      "        page['body'] = response._body\n",
      "        page['title'] = p.title.text\n",
      "        page['depth'] = response.meta['depth']\n",
      "        \n",
      "        # loop through all the hyperlinks\n",
      "        for l in p.find_all('a'):\n",
      "            # if link doesn't have an href skip it\n",
      "            try: \n",
      "                url = l['href']\n",
      "            except KeyError:\n",
      "                continue\n",
      "            \n",
      "            # standardize url\n",
      "            url = self.make_canonical_url(url, current_url)\n",
      "            \n",
      "            # skip mailto addresses\n",
      "            if self.is_mailto(url): continue\n",
      "            # skip cyclic addresses\n",
      "            if self.is_cyclic_url(url): continue\n",
      "            \n",
      "            # add url to urls list for item\n",
      "            anchor_text = l.text  \n",
      "            link = ( url , anchor_text )\n",
      "            #print link\n",
      "            links.append(link)\n",
      "            \n",
      "        page['outLinks'] = links\n",
      "        # add in blank data items, these will get replaced later\n",
      "        page['inLinks'] = []\n",
      "        page['outFiles'] = []\n",
      "        page['inFiles'] = []\n",
      "        # serialize this page\n",
      "        yield page\n",
      "        \n",
      "        # follow the outLinks\n",
      "        for link in page['outLinks']:\n",
      "            web_url = 'http://' + link[0]\n",
      "            if web_url in self.targets_set: yield s.Request(web_url, callback=self.parse, priority=1)\n",
      "            else: yield s.Request(web_url, callback=self.parse)\n",
      "        \n",
      "            \n",
      "    \"\"\"\n",
      "    * Used to take a url as a string and make into canonical form for crawling\n",
      "    * Canonical Rules:\n",
      "    * * Always remove 'http://'  - This will be added back on when submitting a request if using internet\n",
      "    * * Always remove trailing '/'\n",
      "    \"\"\"\n",
      "    def make_canonical_url(self, url, current_url=None):\n",
      "        # change local url to global\n",
      "        if url != '' and url[0] == '/':\n",
      "            url = current_url + url\n",
      "        # remove transer protocol prefixes\n",
      "        if 'https://' in url:\n",
      "            url = url[8:]\n",
      "        if 'http://' in url:\n",
      "            url = url[7:]\n",
      "        # remove trailing slash\n",
      "        if url[-1] == \"/\":\n",
      "            url = url[:-1]\n",
      "        return url\n",
      "    \n",
      "    \"\"\"\n",
      "    * Detect cyclic url pattern\n",
      "    * eg, url = a/b/c/d/e/c/d/e (we use this example for comments)\n",
      "    *         is likely a repeating cycle of links that are valid or is a bot trap\n",
      "    *         either way it won't yield any new content\n",
      "    \"\"\"\n",
      "    def is_cyclic_url(self, url_str):\n",
      "        # make sure url doesn't have trailing slash\n",
      "        if url_str[-1] == '/': url_str = url_str[:-1]\n",
      "        url = url_str.split('/')[::-1] # split into list and reverse order (eg, a/b/c = [c,b,a])\n",
      "        potential_pattern = [url[0]] \n",
      "        for i in range(1,len(url)): # go backwards from second to last towards front\n",
      "            offset =  len(potential_pattern) # size of potential pattern in unexplored prefix of url\n",
      "            if offset >= len(url)/2: # the potential pattern length > half the url, so it can't repeat\n",
      "                return False\n",
      "            elif url[i:i+offset] == potential_pattern: # check for repeated pattern\n",
      "                url = url[i:i+offset]\n",
      "                url.reverse()\n",
      "                print 'Repeating link pattern found: ', url, potential_pattern[::-1]\n",
      "                return True\n",
      "            else: # else char didn't match beginning of tail pattern\n",
      "                potential_pattern.append(url[i]) # add to pattern (eg, tail = [e, d])\n",
      "        return False\n",
      "    \n",
      "    \"\"\"\n",
      "    * Check if the address is a 'mailto' address\n",
      "    \"\"\"\n",
      "    def is_mailto(self, url):\n",
      "        if 'mailto' in url: return True\n",
      "        else: return False\n",
      "        \n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Overwriting findit/spiders/trainDataGatherer.py\n"
       ]
      }
     ],
     "prompt_number": 19
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Run the training data gathering spider"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "### RUN THIS FROM TERMINAL ###\n",
      "#!scrapy crawl trainData "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "The Project Settings File"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%writefile findit/settings.py\n",
      "###############################\n",
      "# -*- coding: utf-8 -*-\n",
      "\n",
      "# Scrapy settings for thesis project\n",
      "#\n",
      "# For simplicity, this file contains only the most important settings by\n",
      "# default. All the other settings are documented here:\n",
      "#\n",
      "#     http://doc.scrapy.org/en/latest/topics/settings.html\n",
      "#\n",
      "\n",
      "BOT_NAME = 'findit'\n",
      "\n",
      "SPIDER_MODULES = ['findit.spiders']\n",
      "NEWSPIDER_MODULE = 'findit.spiders'\n",
      "\n",
      "#CONCURRENT_REQUESTS = 1\n",
      "DEPTH_LIMIT = 8\n",
      "DEPTH_STATS_VERBOSE = True\n",
      "ROBOTSTXT_OBEY = True\n",
      "DOWNLOAD_DELAY = 2\n",
      "LOG = 'INFO'\n",
      "\n",
      "# Make it crawl BFS\n",
      "DEPTH_PRIORITY = 1\n",
      "SCHEDULER_DISK_QUEUE = 'scrapy.squeue.PickleFifoDiskQueue'\n",
      "SCHEDULER_MEMORY_QUEUE = 'scrapy.squeue.FifoMemoryQueue'\n",
      "\n",
      "# ITEM PIPELINES\n",
      "ITEM_PIPELINES = {\n",
      "    'findit.pipelines.DuplicatesPipeline': 100,\n",
      "    'findit.pipelines.JsonWriterPipeline': 1000,\n",
      "}\n",
      "\n",
      "\n",
      "# Crawl responsibly by identifying yourself (and your website) on the user-agent\n",
      "USER_AGENT = 'findit (+http://tomeffland.us)'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Overwriting findit/settings.py\n"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Training Data Item Pipeline"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%writefile findit/pipelines.py\n",
      "###############################\n",
      "# -*- coding: utf-8 -*-\n",
      "#\n",
      "# Don't forget to add your pipeline to the ITEM_PIPELINES setting\n",
      "# See: http://doc.scrapy.org/en/latest/topics/item-pipeline.html\n",
      "import json\n",
      "import os\n",
      "import hashlib\n",
      "from scrapy.exceptions import DropItem\n",
      "\n",
      "\"\"\"\n",
      "* Pipeline to remove duplicate urls\n",
      "\"\"\"\n",
      "class DuplicatesPipeline(object):\n",
      "    def __init__(self):\n",
      "        self.data_dir = 'data/buffalo'\n",
      "        files = os.listdir(self.data_dir)\n",
      "        print \"Files downloaded already: \",\n",
      "        print files\n",
      "        self.urls_seen = set(files) #files\n",
      "        \n",
      "    def process_item(self, item, spider):\n",
      "        if item['url'] in self.urls_seen: \n",
      "            raise DropItem(\"Duplicate item found: %s\" % item['url'])\n",
      "        else:\n",
      "            self.urls_seen.add(item['url'])\n",
      "            return item\n",
      "\n",
      "\"\"\"\n",
      "* Pipeline to write WebPageItem to json file \n",
      "* One file per webpage with url as filename\n",
      "* Serialization technique:\n",
      "*  # OFF # '.' -> '-'\n",
      "*  '/' -> '_'\n",
      "\"\"\"\n",
      "class JsonWriterPipeline(object):\n",
      "    def __init__(self):\n",
      "        self.data_dir = 'data/buffalo'\n",
      "    \n",
      "    def process_item(self, item, spider):\n",
      "        #print \"ITEM URL: \" + item['url']\n",
      "    \n",
      "        # output content\n",
      "        content = {key:item[key] for key in ['title','headers','body','target','url']}\n",
      "        data = json.dumps(dict(content), indent=4) + \"\\n\"\n",
      "        prefix = ''\n",
      "        suffix = '_content'\n",
      "        if content['target']: prefix = 'T_'\n",
      "        \n",
      "        fname = self.make_url_filename(content['url'], prefix=prefix, suffix=suffix)\n",
      "        file = open(fname, 'w')\n",
      "        file.write(data)\n",
      "        file.close()\n",
      "        \n",
      "        # output meta file\n",
      "        meta = {key:item[key] for key in ['url','depth','target','inLinks','outLinks','inFiles','outFiles']}\n",
      "        data = json.dumps(dict(meta), indent=4) + \"\\n\"\n",
      "        prefix = ''\n",
      "        suffix = '_meta'\n",
      "        if content['target']: prefix = 'T_'\n",
      "        \n",
      "        fname = self.make_url_filename(meta['url'], prefix=prefix, suffix=suffix)\n",
      "        file = open(fname, 'w')\n",
      "        file.write(data)\n",
      "        file.close()\n",
      "        \n",
      "        return item\n",
      "    \n",
      "    # change url from example.edu/stuff to example.edu_stuff for a filename\n",
      "    def make_url_filename(self, url, prefix=None, suffix=None):\n",
      "\n",
      "        # use md5 hash hexdigest for filenames since some are too long\n",
      "        url = hashlib.md5(url).hexdigest()\n",
      "        #if target: url = 'T_'+url\n",
      "        url = prefix+url+suffix\n",
      "        print url\n",
      "        return self.data_dir + '/' + url\n",
      "        \n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Overwriting findit/pipelines.py\n"
       ]
      }
     ],
     "prompt_number": 16
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Unit testing the cyclic url identifier function"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def is_cyclic_url( url_str):\n",
      "        # make sure url doesn't have trailing slash\n",
      "        if url_str[-1] == '/': url_str = url_str[:-1]\n",
      "        url = url_str.split('/')[::-1] # split into list and reverse order (eg, a/b/c = [c,b,a])\n",
      "        potential_pattern = [url[0]] \n",
      "        for i in range(1,len(url)): # go backwards from second to last towards front\n",
      "            offset =  len(potential_pattern) # size of potential pattern in unexplored prefix of url\n",
      "            if offset >= len(url)/2: # the potential pattern length > half the url, so it can't repeat\n",
      "                return False\n",
      "            elif url[i:i+offset] == potential_pattern: # check for repeated pattern\n",
      "                url = url[i:i+offset]\n",
      "                url.reverse()\n",
      "                print 'Repeating link pattern found: ', url, potential_pattern[::-1]\n",
      "                return True\n",
      "            else: # else char didn't match beginning of tail pattern\n",
      "                potential_pattern.append(url[i]) # add to pattern (eg, tail = [e, d])\n",
      "        return False\n",
      "    \n",
      "url = 'a/b/c/d/e/c/d/e'\n",
      "url2 = 'a/b/c/d/c/d'\n",
      "url3 = 'a/b/c/d/e/'\n",
      "url4 = 'a/b/c/b/e/d/e'\n",
      "url_test = 'http://www.buffalo.edu/about_ub.html/home/accessibility.html/home/accessibility.html'\n",
      "\n",
      "\n",
      "#print is_cyclic_url(url)\n",
      "#print is_cyclic_url(url2)\n",
      "#print is_cyclic_url(url3)\n",
      "#print is_cyclic_url(url4)\n",
      "print is_cyclic_url(url_test)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "InLink Data Algorithm"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%writefile backlinks.py\n",
      "###########################\n",
      "import os\n",
      "import json\n",
      "import sys\n",
      "\n",
      "if len(sys.argv) == 2:\n",
      "    data_dir = sys.argv[1]\n",
      "else:\n",
      "    print \"ERROR: Please specify the directory to work on\"\n",
      "    quit()\n",
      "\n",
      "files = [ file for file in os.listdir(data_dir) if file[:-5] == \"_meta\" ]\n",
      "\n",
      "inlinks = {} #inlinks is essentially an inverted index of links\n",
      "# get entire inverted index\n",
      "print \"Collecting outgoing links from:\"\n",
      "for fname in files:\n",
      "    f = open(data_dir + '/' + fname, 'r')\n",
      "    #print f.readlines()\n",
      "    data = json.load(f)\n",
      "    url = data['url']\n",
      "    outlinks = data['outLinks']\n",
      "    print url\n",
      "    for link in outlinks:\n",
      "        #print link\n",
      "        try: \n",
      "            if url not in set(inlinks[link[0]]):\n",
      "                inlinks[link[0]].append(url)\n",
      "        except KeyError:\n",
      "            inlinks[link[0]] = []\n",
      "    f.close()\n",
      "print \"Writing back inLinks\"\n",
      "# write the relevant portion of the index back to each file\n",
      "for fname in files:\n",
      "    f = open(data_dir + '/' + fname, 'r+')\n",
      "    data = json.load(f)\n",
      "    url = data['url']\n",
      "    data['inLinks'] = inlinks[url]\n",
      "    #print json.dumps(data)\n",
      "    f.seek(0)\n",
      "    f.write(json.dumps(data))\n",
      "    f.truncate()\n",
      "    f.close()\n",
      "#print inlinks.values()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Overwriting backlinks.py\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# just checkin out the results of the above function\n",
      "import os\n",
      "import json\n",
      "data_dir = 'data/buffalo'\n",
      "files = [ file for file in os.listdir(data_dir) if file[:-5] == \"_meta\" ]\n",
      "\n",
      "inlinks = {} #inlinks is essentially an inverted index of links\n",
      "# get entire inverted index\n",
      "for fname in files:\n",
      "    f = open(data_dir + '/' + fname, 'r')\n",
      "    print fname\n",
      "    data = json.load(f)\n",
      "    print data.keys()\n",
      "\n",
      "    f.close()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!python backlinks.py data/buffalo"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Collecting outgoing links from:\r\n",
        "Writing back inLinks\r\n"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}