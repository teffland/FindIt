{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "!ls -R"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Training Data Spider.ipynb scrapy.cfg                 \u001b[34mthesis\u001b[m\u001b[m\r\n",
        "\r\n",
        "./thesis:\r\n",
        "__init__.py  __init__.pyc items.py     items.pyc    pipelines.py settings.py  settings.pyc \u001b[34mspiders\u001b[m\u001b[m\r\n",
        "\r\n",
        "./thesis/spiders:\r\n",
        "__init__.py     __init__.pyc    basicSpider.py  basicSpider.pyc\r\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Crawl Item Definitions"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#!cat thesis/items.py"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%writefile thesis/items.py\n",
      "#############################\n",
      "# -*- coding: utf-8 -*-\n",
      "\n",
      "# Define here the models for your scraped items\n",
      "#\n",
      "# See documentation in:\n",
      "# http://doc.scrapy.org/en/latest/topics/items.html\n",
      "\n",
      "import scrapy as s\n",
      "\n",
      "\"\"\"\n",
      "Main Webpage class for initial exploration\n",
      "\"\"\"\n",
      "class WebPageItem(s.Item):\n",
      "    title = s.Field()\n",
      "    url = s.Field() # page url\n",
      "    outLinks = s.Field() # a list of (href, anchor-text) pairs\n",
      "    headers = s.Field() # Response.headers\n",
      "    body = s.Field() # body text of page\n",
      "    depth = s.Field() # how many layers away from root we are"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Overwriting thesis/items.py\n"
       ]
      }
     ],
     "prompt_number": 40
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Training Data Spider Defintion:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#!cat thesis/spiders/trainDataGatherer.py"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%writefile thesis/spiders/trainDataGatherer.py\n",
      "###############################################\n",
      "import scrapy as s\n",
      "from bs4 import BeautifulSoup as bs\n",
      "\n",
      "from thesis.items import WebPageItem\n",
      "\n",
      "class trainDataGatherer(s.Spider):\n",
      "    name = \"trainData\"\n",
      "\n",
      "    def __init__(self, *a, **kw):\n",
      "        self.allowed_domains = [\"buffalo.edu\"]\n",
      "        self.start_urls = [ \"http://www.buffalo.edu\" ]\n",
      "  \n",
      "        super(trainDataGatherer, self).__init__(*a, **kw)\n",
      "    \n",
      "    def parse(self, response):\n",
      "        # parse the page with bs4\n",
      "        p = bs(response._body)\n",
      "\n",
      "        # the page item\n",
      "        page  = WebPageItem()\n",
      "        links = []\n",
      "        \n",
      "        # standardize current page url\n",
      "        current_url = self.make_canonical_url(response._url)\n",
      "        \n",
      "        # populate page data item\n",
      "        page['url'] = current_url # add url to page object\n",
      "        page['headers'] = response.headers\n",
      "        page['body'] = response._body\n",
      "        page['title'] = p.title.text\n",
      "        page['depth'] = response.meta['depth']\n",
      "        \n",
      "        # loop through all the hyperlinks\n",
      "        for l in p.find_all('a'):\n",
      "            # if link doesn't have an href skip it\n",
      "            try: \n",
      "                url = l['href']\n",
      "            except KeyError:\n",
      "                continue\n",
      "            \n",
      "            # standardize url\n",
      "            url = self.make_canonical_url(url, current_url)\n",
      "            \n",
      "            # skip mailto addresses\n",
      "            if self.is_mailto(url): continue\n",
      "            # skip cyclic addresses\n",
      "            if self.is_cyclic_url(url): continue\n",
      "            \n",
      "            # add url to urls list for item\n",
      "            anchor_text = l.text  \n",
      "            link = ( url , anchor_text )\n",
      "            #print link\n",
      "            links.append(link)\n",
      "            \n",
      "        page['outLinks'] = links\n",
      "        \n",
      "        # serialize this page\n",
      "        yield page\n",
      "        \n",
      "        # follow the outLinks\n",
      "        for link in page['outLinks']:\n",
      "            web_url = 'http://' + link[0]\n",
      "            yield s.Request(web_url, callback=self.parse)\n",
      "        \n",
      "            \n",
      "    \"\"\"\n",
      "    * Used to take a url as a string and make into canonical form for crawling\n",
      "    * Canonical Rules:\n",
      "    * * Always remove 'http://'  - This will be added back on when submitting a request if using internet\n",
      "    * * Always remove trailing '/'\n",
      "    \"\"\"\n",
      "    def make_canonical_url(self, url, current_url=None):\n",
      "        # change local url to global\n",
      "        if url != '' and url[0] == '/':\n",
      "            url = current_url + url\n",
      "        # remove transer protocol prefixes\n",
      "        if 'https://' in url:\n",
      "            url = url[8:]\n",
      "        if 'http://' in url:\n",
      "            url = url[7:]\n",
      "        # remove trailing slash\n",
      "        if url[-1] == \"/\":\n",
      "            url = url[:-1]\n",
      "        return url\n",
      "    \n",
      "    \"\"\"\n",
      "    * Detect cyclic url pattern\n",
      "    * eg, url = a/b/c/d/e/c/d/e (we use this example for comments)\n",
      "    *         is likely a repeating cycle of links that are valid or is a bot trap\n",
      "    *         either way it won't yield any new content\n",
      "    \"\"\"\n",
      "    def is_cyclic_url(self, url_str):\n",
      "        # make sure url doesn't have trailing slash\n",
      "        if url_str[-1] == '/': url_str = url_str[:-1]\n",
      "        url = url_str.split('/')[::-1] # split into list and reverse order (eg, a/b/c = [c,b,a])\n",
      "        potential_pattern = [url[0]] \n",
      "        for i in range(1,len(url)): # go backwards from second to last towards front\n",
      "            offset =  len(potential_pattern) # size of potential pattern in unexplored prefix of url\n",
      "            if offset >= len(url)/2: # the potential pattern length > half the url, so it can't repeat\n",
      "                return False\n",
      "            elif url[i:i+offset] == potential_pattern: # check for repeated pattern\n",
      "                url = url[i:i+offset]\n",
      "                url.reverse()\n",
      "                print 'Repeating link pattern found: ', url, potential_pattern[::-1]\n",
      "                return True\n",
      "            else: # else char didn't match beginning of tail pattern\n",
      "                potential_pattern.append(url[i]) # add to pattern (eg, tail = [e, d])\n",
      "        return False\n",
      "    \n",
      "    \"\"\"\n",
      "    * Check if the address is a 'mailto' address\n",
      "    \"\"\"\n",
      "    def is_mailto(self, url):\n",
      "        if 'mailto' in url: return True\n",
      "        else: return False\n",
      "        \n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Overwriting thesis/spiders/trainDataGatherer.py\n"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Run the training data gathering spider"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "### RUN THIS FROM TERMINAL ###\n",
      "#!scrapy crawl trainData "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "The Project Settings File"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#!cat thesis/settings.py"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 38
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%writefile thesis/settings.py\n",
      "###############################\n",
      "# -*- coding: utf-8 -*-\n",
      "\n",
      "# Scrapy settings for thesis project\n",
      "#\n",
      "# For simplicity, this file contains only the most important settings by\n",
      "# default. All the other settings are documented here:\n",
      "#\n",
      "#     http://doc.scrapy.org/en/latest/topics/settings.html\n",
      "#\n",
      "\n",
      "BOT_NAME = 'thesis'\n",
      "\n",
      "SPIDER_MODULES = ['thesis.spiders']\n",
      "NEWSPIDER_MODULE = 'thesis.spiders'\n",
      "\n",
      "#CONCURRENT_REQUESTS = 1\n",
      "#DEPTH_LIMIT = 1\n",
      "DEPTH_STATS_VERBOSE = True\n",
      "ROBOTSTXT_OBEY = True\n",
      "DOWNLOAD_DELAY = 2\n",
      "LOG = 'INFO'\n",
      "\n",
      "# ITEM PIPELINES\n",
      "ITEM_PIPELINES = {\n",
      "    'thesis.pipelines.DuplicatesPipeline': 100,\n",
      "    'thesis.pipelines.JsonWriterPipeline': 1000,\n",
      "}\n",
      "\n",
      "\n",
      "# Crawl responsibly by identifying yourself (and your website) on the user-agent\n",
      "#USER_AGENT = 'thesis (+http://www.yourdomain.com)'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Overwriting thesis/settings.py\n"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Training Data Item Pipeline"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#!cat thesis/pipelines.py"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 39
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%writefile thesis/pipelines.py\n",
      "###############################\n",
      "# -*- coding: utf-8 -*-\n",
      "#\n",
      "# Don't forget to add your pipeline to the ITEM_PIPELINES setting\n",
      "# See: http://doc.scrapy.org/en/latest/topics/item-pipeline.html\n",
      "import json\n",
      "import os\n",
      "import hashlib\n",
      "from scrapy.exceptions import DropItem\n",
      "\n",
      "\"\"\"\n",
      "* Pipeline to remove duplicate urls\n",
      "\"\"\"\n",
      "class DuplicatesPipeline(object):\n",
      "    def __init__(self):\n",
      "        self.data_dir = 'data/buffalo'\n",
      "        files = os.listdir(self.data_dir)\n",
      "        print \"Files downloaded already: \",\n",
      "        print files\n",
      "        self.urls_seen = set(files) #files\n",
      "        \n",
      "    def process_item(self, item, spider):\n",
      "        if item['url'] in self.urls_seen: \n",
      "            raise DropItem(\"Duplicate item found: %s\" % item['url'])\n",
      "        else:\n",
      "            self.urls_seen.add(item['url'])\n",
      "            return item\n",
      "\n",
      "\"\"\"\n",
      "* Pipeline to write WebPageItem to json file \n",
      "* One file per webpage with url as filename\n",
      "* Serialization technique:\n",
      "*  # OFF # '.' -> '-'\n",
      "*  '/' -> '_'\n",
      "\"\"\"\n",
      "class JsonWriterPipeline(object):\n",
      "    def __init__(self):\n",
      "        self.data_dir = 'data/buffalo'\n",
      "\n",
      "    def process_item(self, item, spider):\n",
      "        data = json.dumps(dict(item), indent=4) + \"\\n\"\n",
      "        print \"ITEM URL: \" + item['url']\n",
      "        fname = self.make_url_filename(item['url'])\n",
      "        file = open(fname, 'w')\n",
      "        file.write(data)\n",
      "        file.close()\n",
      "        return item\n",
      "    \n",
      "    # change url from example.edu/stuff to example.edu_stuff for a filename\n",
      "    def make_url_filename(self, url):\n",
      "        #url.replace('.', '-')\n",
      "        #url = url.replace('/', '_')\n",
      "        \n",
      "        # use md5 hash hexdigest for filenames since some are too long\n",
      "        url = hashlib.md5(url).hexdigest()\n",
      "        print url\n",
      "        return self.data_dir + '/' + url\n",
      "        \n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Overwriting thesis/pipelines.py\n"
       ]
      }
     ],
     "prompt_number": 41
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Unit testing the cyclic url identifier function"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def is_cyclic_url( url_str):\n",
      "        # make sure url doesn't have trailing slash\n",
      "        if url_str[-1] == '/': url_str = url_str[:-1]\n",
      "        url = url_str.split('/')[::-1] # split into list and reverse order (eg, a/b/c = [c,b,a])\n",
      "        potential_pattern = [url[0]] \n",
      "        for i in range(1,len(url)): # go backwards from second to last towards front\n",
      "            offset =  len(potential_pattern) # size of potential pattern in unexplored prefix of url\n",
      "            if offset >= len(url)/2: # the potential pattern length > half the url, so it can't repeat\n",
      "                return False\n",
      "            elif url[i:i+offset] == potential_pattern: # check for repeated pattern\n",
      "                url = url[i:i+offset]\n",
      "                url.reverse()\n",
      "                print 'Repeating link pattern found: ', url, potential_pattern[::-1]\n",
      "                return True\n",
      "            else: # else char didn't match beginning of tail pattern\n",
      "                potential_pattern.append(url[i]) # add to pattern (eg, tail = [e, d])\n",
      "        return False\n",
      "    \n",
      "url = 'a/b/c/d/e/c/d/e'\n",
      "url2 = 'a/b/c/d/c/d'\n",
      "url3 = 'a/b/c/d/e/'\n",
      "url4 = 'a/b/c/b/e/d/e'\n",
      "url_test = 'http://www.buffalo.edu/about_ub.html/home/accessibility.html/home/accessibility.html'\n",
      "\n",
      "\n",
      "#print is_cyclic_url(url)\n",
      "#print is_cyclic_url(url2)\n",
      "#print is_cyclic_url(url3)\n",
      "#print is_cyclic_url(url4)\n",
      "print is_cyclic_url(url_test)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "InLink Data Algorithm"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%writefile backlinks.py\n",
      "###########################\n",
      "import os\n",
      "import json\n",
      "import sys\n",
      "\n",
      "if len(sys.argv) == 2:\n",
      "    data_dir = sys.argv[1]\n",
      "else:\n",
      "    print \"ERROR: Please specify the directory to work on\"\n",
      "    quit()\n",
      "\n",
      "files = os.listdir(data_dir)\n",
      "\n",
      "inlinks = {} #inlinks is essentially an inverted index of links\n",
      "# get entire inverted index\n",
      "print \"Collecting outgoing links from:\"\n",
      "for fname in files:\n",
      "    f = open(data_dir + '/' + fname, 'r')\n",
      "    #print f.readlines()\n",
      "    data = json.load(f)\n",
      "    url = data['url']\n",
      "    outlinks = data['outLinks']\n",
      "    print url\n",
      "    for link in outlinks:\n",
      "        #print link\n",
      "        try: \n",
      "            if url not in set(inlinks[link[0]]):\n",
      "                inlinks[link[0]].append(url)\n",
      "        except KeyError:\n",
      "            inlinks[link[0]] = []\n",
      "    f.close()\n",
      "print \"Writing back inLinks\"\n",
      "# write the relevant portion of the index back to each file\n",
      "for fname in files:\n",
      "    f = open(data_dir + '/' + fname, 'r+')\n",
      "    data = json.load(f)\n",
      "    url = data['url']\n",
      "    data['inLinks'] = inlinks[url]\n",
      "    #print json.dumps(data)\n",
      "    f.seek(0)\n",
      "    f.write(json.dumps(data))\n",
      "    f.truncate()\n",
      "    f.close()\n",
      "#print inlinks.values()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Overwriting backlinks.py\n"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# just checkin out the results of the above function\n",
      "import os\n",
      "import json\n",
      "data_dir = 'data/buffalo'\n",
      "files = os.listdir(data_dir)\n",
      "\n",
      "inlinks = {} #inlinks is essentially an inverted index of links\n",
      "# get entire inverted index\n",
      "for fname in files:\n",
      "    f = open(data_dir + '/' + fname, 'r')\n",
      "    print fname\n",
      "    data = json.load(f)\n",
      "    print data.keys()\n",
      "\n",
      "    f.close()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0c7131fe299e9900dd6b06b548e5000d\n",
        "[u'body', u'inLinks', u'url', u'title', u'headers', u'depth', u'outLinks']\n",
        "33935dd1fd75e92906d6acae4be65b59\n",
        "[u'body', u'inLinks', u'url', u'title', u'headers', u'depth', u'outLinks']\n",
        "512a3b959ff757fd284ceab78003dea8\n",
        "[u'body', u'inLinks', u'url', u'title', u'headers', u'depth', u'outLinks']\n",
        "5cbd36acb79711248839127d652ff472\n",
        "[u'body', u'inLinks', u'url', u'title', u'headers', u'depth', u'outLinks']\n",
        "7887a179edaae6972fbe58e1d86a146c\n",
        "[u'body', u'inLinks', u'url', u'title', u'headers', u'depth', u'outLinks']\n",
        "9bb7ddecef7e88e58cf033b2ce47f752\n",
        "[u'body', u'inLinks', u'url', u'title', u'headers', u'depth', u'outLinks']\n",
        "9e73b13ea734ea3744688cc5930456e6\n",
        "[u'body', u'inLinks', u'url', u'title', u'headers', u'depth', u'outLinks']\n",
        "cb94eeed6c6005fbf26770d11cb5daf6\n",
        "[u'body', u'inLinks', u'url', u'title', u'headers', u'depth', u'outLinks']\n",
        "d58f5d425857936371c3f966bad2da1d\n",
        "[u'body', u'inLinks', u'url', u'title', u'headers', u'depth', u'outLinks']\n",
        "e0b1c3f943204000c67e0a5a4b06d42b\n",
        "[u'body', u'inLinks', u'url', u'title', u'headers', u'depth', u'outLinks']\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}